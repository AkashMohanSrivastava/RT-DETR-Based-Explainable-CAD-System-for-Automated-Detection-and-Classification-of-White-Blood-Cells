{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RT-DETR-X Training\n",
    "\n",
    "Training RT-DETR-X with ResNet-101 backbone for WBC Classification on Raabin-WBC dataset.\n",
    "\n",
    "## Model Details\n",
    "- **Backbone**: ResNet-101\n",
    "- **Training**: Pretrained weights (fine-tuning)\n",
    "- **Dataset**: Raabin-WBC with 5 cell types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install -U ultralytics torch torchvision pillow tqdm scikit-learn seaborn timm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "%matplotlib inline\n\nimport os\nimport json\nimport yaml\nfrom datetime import datetime\n\nimport numpy as np\nimport torch\n\nfrom sklearn.metrics import classification_report\n\n# Import common training utilities\nfrom training_utils import (\n    create_sampled_dataset,\n    create_full_dataset_config,\n    train_model,\n    evaluate_model,\n    save_results,\n    print_training_summary,\n)\n\nprint(f\"PyTorch version: {torch.__version__}\")\nprint(f\"CUDA available: {torch.cuda.is_available()}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# =============================================================================\n# MODEL CONFIGURATION\n# =============================================================================\nMODEL_NAME = \"RT-DETR-X\"\nBACKBONE = \"ResNet-101\"\nIS_PRETRAINED = True  # Using pretrained weights\n\n# Pretrained model file\nMODEL_FILE = \"rtdetr-x.pt\"\n\n# =============================================================================\n# BASE DIRECTORY\n# =============================================================================\nNOTEBOOK_DIR = os.getcwd()\nBASE_DIR = os.path.join(NOTEBOOK_DIR, \"output\")\n\n# Dataset path (contains separate Train and val folders)\nDATA_ROOT = r\"C:\\D drive\\mydata\\MSML\\DataSets\\Raabin_datsets_withlabels\"\n\nprint(f\"Notebook directory: {NOTEBOOK_DIR}\")\nprint(f\"Base directory: {BASE_DIR}\")\nprint(f\"Data root: {DATA_ROOT}\")\n\n# =============================================================================\n# SAMPLING CONFIGURATION\n# =============================================================================\nUSE_FULL_DATASET = True  # Set to True to use ALL images, False for sampling\n\n# Sample sizes per class (only used when USE_FULL_DATASET=False)\nTRAIN_SAMPLE_SIZE = 100   # Number of training samples per class\nVAL_SAMPLE_SIZE = 20      # Number of validation samples per class\n\n# =============================================================================\n# CHECKPOINT CONFIGURATION (for resume training on full dataset)\n# =============================================================================\nCHECKPOINT_DIR = os.path.join(BASE_DIR, \"checkpoints\", MODEL_NAME)\nCHECKPOINT_MODEL_PATH = os.path.join(CHECKPOINT_DIR, \"last.pt\")\nCHECKPOINT_META_PATH = os.path.join(CHECKPOINT_DIR, \"training_meta.json\")\n\n# Create checkpoint directory\nos.makedirs(CHECKPOINT_DIR, exist_ok=True)\n\n# Data paths (separate train and validation directories)\nTRAIN_IMAGES_DIR = os.path.join(DATA_ROOT, \"Train\", \"images\")\nTRAIN_LABELS_DIR = os.path.join(DATA_ROOT, \"Train\", \"labels\")\nVAL_IMAGES_DIR = os.path.join(DATA_ROOT, \"val\", \"images\")\nVAL_LABELS_DIR = os.path.join(DATA_ROOT, \"val\", \"labels\")\n\n# For evaluation (uses training images by default)\nIMAGES_DIR = TRAIN_IMAGES_DIR\n\n# Output directories\nos.makedirs(BASE_DIR, exist_ok=True)\nMODEL_DIR = os.path.join(BASE_DIR, \"models\")\nRESULTS_DIR = os.path.join(BASE_DIR, \"results\")\nos.makedirs(MODEL_DIR, exist_ok=True)\nos.makedirs(RESULTS_DIR, exist_ok=True)\n\n# Device configuration\nDEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n# Class definitions\nCLASSES = {\n    \"Basophil\": 0,\n    \"Eosinophil\": 1,\n    \"Lymphocyte\": 2,\n    \"Monocyte\": 3,\n    \"Neutrophil\": 4\n}\nID2LABEL = {v: k for k, v in CLASSES.items()}\nNUM_CLASSES = len(CLASSES)\n\nprint(f\"\\nUsing device: {DEVICE}\")\nif USE_FULL_DATASET:\n    print(f\"Dataset mode: FULL DATASET\")\n    print(f\"Checkpoint directory: {CHECKPOINT_DIR}\")\n    # Check for existing checkpoint\n    if os.path.exists(CHECKPOINT_MODEL_PATH) and os.path.exists(CHECKPOINT_META_PATH):\n        with open(CHECKPOINT_META_PATH, 'r') as f:\n            meta = json.load(f)\n        print(f\"  -> Found existing checkpoint: {meta['total_epochs']} epochs completed\")\n        print(f\"  -> Training will RESUME from epoch {meta['total_epochs'] + 1}\")\n    else:\n        print(f\"  -> No checkpoint found. Training will start from scratch.\")\nelse:\n    print(f\"Dataset mode: SAMPLED (Train: {TRAIN_SAMPLE_SIZE}/class, Val: {VAL_SAMPLE_SIZE}/class)\")\n    print(f\"  -> Sampled mode: Always starts fresh (no resume)\")\nprint(f\"\\nTraining data: {TRAIN_IMAGES_DIR}\")\nprint(f\"Validation data: {VAL_IMAGES_DIR}\")\nprint(f\"\\nModel: {MODEL_NAME} ({BACKBONE})\")\nprint(f\"Training mode: {'Pretrained (fine-tuning)' if IS_PRETRAINED else 'From scratch'}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Training Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Configuration (Pretrained Fine-tuning):\n",
      "============================================================\n",
      "  epochs: 1\n",
      "  imgsz: 640\n",
      "  batch: 4\n",
      "  lr0: 0.01\n",
      "  lrf: 0.0001\n",
      "  momentum: 0.937\n",
      "  weight_decay: 0.0005\n",
      "  workers: 8\n",
      "  patience: 15\n",
      "  cos_lr: True\n",
      "  warmup_epochs: 1\n",
      "  warmup_momentum: 0.8\n",
      "  warmup_bias_lr: 0.1\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# TRAINING HYPERPARAMETERS (PRETRAINED CONFIG)\n",
    "# =============================================================================\n",
    "# Fewer epochs needed since backbone is already trained\n",
    "\n",
    "TRAINING_CONFIG = {\n",
    "    \"epochs\": 1,            \n",
    "    \"imgsz\": 640,\n",
    "    \"batch\": 4,\n",
    "    \"lr0\": 0.01,            \n",
    "    \"lrf\": 0.0001,\n",
    "    \"momentum\": 0.937,\n",
    "    \"weight_decay\": 0.0005,\n",
    "    \"workers\": 8,\n",
    "    \"patience\": 15,\n",
    "    \"cos_lr\": True,\n",
    "    \"warmup_epochs\": 1,\n",
    "    \"warmup_momentum\": 0.8,\n",
    "    \"warmup_bias_lr\": 0.1,\n",
    "}\n",
    "\n",
    "print(\"Training Configuration (Pretrained Fine-tuning):\")\n",
    "print(\"=\"*60)\n",
    "for k, v in TRAINING_CONFIG.items():\n",
    "    print(f\"  {k}: {v}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# create_sampled_dataset is imported from training_utils.py\n# See training_utils.py for the implementation"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Create data configuration\nif USE_FULL_DATASET:\n    print(\"Using FULL DATASET\\n\")\n    print(f\"Training: {TRAIN_IMAGES_DIR}\")\n    print(f\"Validation: {VAL_IMAGES_DIR}\")\n    \n    DATA_YAML = create_full_dataset_config(DATA_ROOT, BASE_DIR, NUM_CLASSES, ID2LABEL)\n    print(f\"\\nData config: {DATA_YAML}\")\nelse:\n    print(f\"Creating SAMPLED dataset...\")\n    print(f\"  Train samples: {TRAIN_SAMPLE_SIZE} per class\")\n    print(f\"  Val samples: {VAL_SAMPLE_SIZE} per class\\n\")\n    \n    DATA_YAML = create_sampled_dataset(\n        DATA_ROOT, \n        BASE_DIR, \n        CLASSES, \n        train_samples_per_class=TRAIN_SAMPLE_SIZE,\n        val_samples_per_class=VAL_SAMPLE_SIZE,\n        random_seed=42\n    )"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# train_model is imported from training_utils.py\n# See training_utils.py for the implementation"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Train the model\ntraining_result = train_model(\n    model_source=MODEL_FILE,\n    model_name=MODEL_NAME,\n    data_yaml=DATA_YAML,\n    training_config=TRAINING_CONFIG,\n    base_dir=BASE_DIR,\n    use_full_dataset=USE_FULL_DATASET,\n    checkpoint_dir=CHECKPOINT_DIR if USE_FULL_DATASET else None,\n    default_warmup_epochs=1  # Pretrained model needs less warmup\n)\n\nprint(f\"\\nTraining completed in {training_result['training_time']:.1f}s\")\nprint(f\"Best model saved to: {training_result['best_model_path']}\")\n\nif training_result['resumed']:\n    print(f\"\\nResumed from epoch {training_result['previous_epochs'] + 1}\")\nprint(f\"Total epochs trained: {training_result['total_epochs']}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# evaluate_model is imported from training_utils.py\n# See training_utils.py for the implementation"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating: RT-DETR-X\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Results:\n",
      "  Accuracy: 0.8160\n",
      "  Avg inference time: 53.54ms\n",
      "  No predictions: 0/500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "CONF_THRESH = 0.1\n",
    "EVAL_PER_CLASS = 100\n",
    "\n",
    "print(f\"Evaluating: {MODEL_NAME}\")\n",
    "evaluation_result = evaluate_model(\n",
    "    model_path=training_result[\"best_model_path\"],\n",
    "    images_dir=IMAGES_DIR,\n",
    "    classes=CLASSES,\n",
    "    id2label=ID2LABEL,\n",
    "    conf_thresh=CONF_THRESH,\n",
    "    eval_per_class=EVAL_PER_CLASS,\n",
    ")\n",
    "\n",
    "print(f\"\\nResults:\")\n",
    "print(f\"  Accuracy: {evaluation_result['accuracy']:.4f}\")\n",
    "print(f\"  Avg inference time: {evaluation_result['avg_inference_time']*1000:.2f}ms\")\n",
    "print(f\"  No predictions: {evaluation_result['no_prediction_count']}/{evaluation_result['total_samples']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- RT-DETR-X Classification Report ---\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Basophil       0.65      1.00      0.79       100\n",
      "  Eosinophil       0.94      0.81      0.87       100\n",
      "  Lymphocyte       0.98      0.63      0.77       100\n",
      "    Monocyte       0.76      0.91      0.83       100\n",
      "  Neutrophil       0.96      0.73      0.83       100\n",
      "\n",
      "    accuracy                           0.82       500\n",
      "   macro avg       0.86      0.82      0.82       500\n",
      "weighted avg       0.86      0.82      0.82       500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print classification report\n",
    "if evaluation_result[\"classification_report\"] is not None:\n",
    "    y_true = np.array(evaluation_result[\"y_true\"])\n",
    "    y_pred = np.array(evaluation_result[\"y_pred\"])\n",
    "    valid = y_pred != -1\n",
    "    \n",
    "    print(f\"\\n--- {MODEL_NAME} Classification Report ---\")\n",
    "    print(classification_report(\n",
    "        y_true[valid],\n",
    "        y_pred[valid],\n",
    "        target_names=list(CLASSES.keys()),\n",
    "        labels=list(range(NUM_CLASSES)),\n",
    "        zero_division=0\n",
    "    ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Save Results to Disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Save results to JSON\nresults_file = save_results(\n    results_dir=RESULTS_DIR,\n    model_name=MODEL_NAME,\n    backbone=BACKBONE,\n    is_pretrained=IS_PRETRAINED,\n    training_result=training_result,\n    evaluation_result=evaluation_result,\n    training_config=TRAINING_CONFIG,\n    classes=CLASSES\n)\n\nprint(f\"Results saved to: {results_file}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Print training summary\nprint_training_summary(\n    model_name=MODEL_NAME,\n    backbone=BACKBONE,\n    training_result=training_result,\n    evaluation_result=evaluation_result,\n    training_config=TRAINING_CONFIG,\n    checkpoint_model_path=CHECKPOINT_MODEL_PATH if USE_FULL_DATASET else None,\n    results_file=results_file\n)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}