{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RT-DETR MobileNet Training\n",
    "\n",
    "Training RT-DETR with MobileNetV3-Small backbone for WBC Classification on Raabin-WBC dataset.\n",
    "\n",
    "## Model Details\n",
    "- **Backbone**: MobileNetV3-Small\n",
    "- **Training**: From scratch (no pretrained weights)\n",
    "- **Dataset**: Raabin-WBC with 5 cell types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install -U ultralytics torch torchvision pillow tqdm scikit-learn seaborn timm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.6.0+cu124\n",
      "CUDA available: True\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "import json\n",
    "import random\n",
    "import shutil\n",
    "import yaml\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "from ultralytics import RTDETR\n",
    "\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Notebook directory: C:\\D drive\\mydata\\MSML\\GitHub\\RT-DETR-Based-Explainable-CAD-System-for-Automated-Detection-and-Classification-of-White-Blood-Cells\n",
      "Base directory: C:\\D drive\\mydata\\MSML\\GitHub\\RT-DETR-Based-Explainable-CAD-System-for-Automated-Detection-and-Classification-of-White-Blood-Cells\\output\n",
      "Data root: C:\\D drive\\mydata\\MSML\\DataSets\\Raabin_datsets_withlabels\n",
      "Found model YAML: C:\\D drive\\mydata\\MSML\\GitHub\\RT-DETR-Based-Explainable-CAD-System-for-Automated-Detection-and-Classification-of-White-Blood-Cells\\rtdetr_mobilenetv3.yaml\n",
      "\n",
      "Using device: cuda\n",
      "Dataset mode: FULL DATASET\n",
      "Checkpoint directory: C:\\D drive\\mydata\\MSML\\GitHub\\RT-DETR-Based-Explainable-CAD-System-for-Automated-Detection-and-Classification-of-White-Blood-Cells\\output\\checkpoints\\RT-DETR-MobileNet\n",
      "  -> No checkpoint found. Training will start from scratch.\n",
      "\n",
      "Training data: C:\\D drive\\mydata\\MSML\\DataSets\\Raabin_datsets_withlabels\\Train\\images\n",
      "Validation data: C:\\D drive\\mydata\\MSML\\DataSets\\Raabin_datsets_withlabels\\val\\images\n",
      "\n",
      "Model: RT-DETR-MobileNet (MobileNetV3-Small)\n",
      "Training mode: From scratch\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# MODEL CONFIGURATION\n",
    "# =============================================================================\n",
    "MODEL_NAME = \"RT-DETR-MobileNet\"\n",
    "BACKBONE = \"MobileNetV3-Small\"\n",
    "IS_PRETRAINED = False  # Training from scratch\n",
    "\n",
    "# =============================================================================\n",
    "# BASE DIRECTORY\n",
    "# =============================================================================\n",
    "NOTEBOOK_DIR = os.getcwd()\n",
    "BASE_DIR = os.path.join(NOTEBOOK_DIR, \"output\")\n",
    "\n",
    "# Dataset path (contains separate Train and val folders)\n",
    "DATA_ROOT = r\"C:\\D drive\\mydata\\MSML\\DataSets\\Raabin_datsets_withlabels\"\n",
    "\n",
    "# Custom model YAML path\n",
    "MODEL_YAML_PATH = os.path.join(NOTEBOOK_DIR, \"rtdetr_mobilenetv3.yaml\")\n",
    "\n",
    "print(f\"Notebook directory: {NOTEBOOK_DIR}\")\n",
    "print(f\"Base directory: {BASE_DIR}\")\n",
    "print(f\"Data root: {DATA_ROOT}\")\n",
    "\n",
    "# Verify YAML file exists\n",
    "if os.path.exists(MODEL_YAML_PATH):\n",
    "    print(f\"Found model YAML: {MODEL_YAML_PATH}\")\n",
    "else:\n",
    "    print(f\"WARNING: Model YAML not found at: {MODEL_YAML_PATH}\")\n",
    "\n",
    "# =============================================================================\n",
    "# SAMPLING CONFIGURATION\n",
    "# =============================================================================\n",
    "USE_FULL_DATASET = True  # Set to True to use ALL images, False for sampling\n",
    "\n",
    "# Sample sizes per class (only used when USE_FULL_DATASET=False)\n",
    "TRAIN_SAMPLE_SIZE = 100   # Number of training samples per class\n",
    "VAL_SAMPLE_SIZE = 20      # Number of validation samples per class\n",
    "\n",
    "# =============================================================================\n",
    "# CHECKPOINT CONFIGURATION (for resume training on full dataset)\n",
    "# =============================================================================\n",
    "CHECKPOINT_DIR = os.path.join(BASE_DIR, \"checkpoints\", MODEL_NAME)\n",
    "CHECKPOINT_MODEL_PATH = os.path.join(CHECKPOINT_DIR, \"last.pt\")\n",
    "CHECKPOINT_META_PATH = os.path.join(CHECKPOINT_DIR, \"training_meta.json\")\n",
    "\n",
    "# Create checkpoint directory\n",
    "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
    "\n",
    "# Data paths (separate train and validation directories)\n",
    "TRAIN_IMAGES_DIR = os.path.join(DATA_ROOT, \"Train\", \"images\")\n",
    "TRAIN_LABELS_DIR = os.path.join(DATA_ROOT, \"Train\", \"labels\")\n",
    "VAL_IMAGES_DIR = os.path.join(DATA_ROOT, \"val\", \"images\")\n",
    "VAL_LABELS_DIR = os.path.join(DATA_ROOT, \"val\", \"labels\")\n",
    "\n",
    "# For evaluation (uses training images by default)\n",
    "IMAGES_DIR = TRAIN_IMAGES_DIR\n",
    "\n",
    "# Output directories\n",
    "os.makedirs(BASE_DIR, exist_ok=True)\n",
    "MODEL_DIR = os.path.join(BASE_DIR, \"models\")\n",
    "RESULTS_DIR = os.path.join(BASE_DIR, \"results\")\n",
    "os.makedirs(MODEL_DIR, exist_ok=True)\n",
    "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
    "\n",
    "# Device configuration\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Class definitions\n",
    "CLASSES = {\n",
    "    \"Basophil\": 0,\n",
    "    \"Eosinophil\": 1,\n",
    "    \"Lymphocyte\": 2,\n",
    "    \"Monocyte\": 3,\n",
    "    \"Neutrophil\": 4\n",
    "}\n",
    "ID2LABEL = {v: k for k, v in CLASSES.items()}\n",
    "NUM_CLASSES = len(CLASSES)\n",
    "\n",
    "print(f\"\\nUsing device: {DEVICE}\")\n",
    "if USE_FULL_DATASET:\n",
    "    print(f\"Dataset mode: FULL DATASET\")\n",
    "    print(f\"Checkpoint directory: {CHECKPOINT_DIR}\")\n",
    "    # Check for existing checkpoint\n",
    "    if os.path.exists(CHECKPOINT_MODEL_PATH) and os.path.exists(CHECKPOINT_META_PATH):\n",
    "        with open(CHECKPOINT_META_PATH, 'r') as f:\n",
    "            meta = json.load(f)\n",
    "        print(f\"  -> Found existing checkpoint: {meta['total_epochs']} epochs completed\")\n",
    "        print(f\"  -> Training will RESUME from epoch {meta['total_epochs'] + 1}\")\n",
    "    else:\n",
    "        print(f\"  -> No checkpoint found. Training will start from scratch.\")\n",
    "else:\n",
    "    print(f\"Dataset mode: SAMPLED (Train: {TRAIN_SAMPLE_SIZE}/class, Val: {VAL_SAMPLE_SIZE}/class)\")\n",
    "    print(f\"  -> Sampled mode: Always starts fresh (no resume)\")\n",
    "print(f\"\\nTraining data: {TRAIN_IMAGES_DIR}\")\n",
    "print(f\"Validation data: {VAL_IMAGES_DIR}\")\n",
    "print(f\"\\nModel: {MODEL_NAME} ({BACKBONE})\")\n",
    "print(f\"Training mode: {'Pretrained' if IS_PRETRAINED else 'From scratch'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Training Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Configuration:\n",
      "============================================================\n",
      "  epochs: 1\n",
      "  imgsz: 640\n",
      "  batch: 8\n",
      "  lr0: 0.0001\n",
      "  lrf: 0.01\n",
      "  momentum: 0.937\n",
      "  weight_decay: 0.0005\n",
      "  workers: 8\n",
      "  patience: 20\n",
      "  cos_lr: True\n",
      "  warmup_epochs: 3\n",
      "  warmup_momentum: 0.8\n",
      "  warmup_bias_lr: 0.01\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# TRAINING HYPERPARAMETERS (FROM SCRATCH CONFIG)\n",
    "# =============================================================================\n",
    "# Training from scratch needs more epochs and lower learning rate\n",
    "\n",
    "TRAINING_CONFIG = {\n",
    "    \"epochs\": 1,           \n",
    "    \"imgsz\": 640,\n",
    "    \"batch\": 8,\n",
    "    \"lr0\": 0.0001,          # Low LR for stability\n",
    "    \"lrf\": 0.01,\n",
    "    \"momentum\": 0.937,\n",
    "    \"weight_decay\": 0.0005,\n",
    "    \"workers\": 8,\n",
    "    \"patience\": 20,\n",
    "    \"cos_lr\": True,\n",
    "    \"warmup_epochs\": 3,     # Gradual LR ramp-up for stability\n",
    "    \"warmup_momentum\": 0.8,\n",
    "    \"warmup_bias_lr\": 0.01,\n",
    "}\n",
    "\n",
    "print(\"Training Configuration:\")\n",
    "print(\"=\"*60)\n",
    "for k, v in TRAINING_CONFIG.items():\n",
    "    print(f\"  {k}: {v}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sampled_dataset(data_root, base_dir, classes, train_samples_per_class, val_samples_per_class, random_seed=42):\n",
    "    \"\"\"\n",
    "    Create a sampled dataset from separate Train and val directories.\n",
    "    - Samples from Train directory for training data\n",
    "    - Samples from val directory for validation data (NOT from Train)\n",
    "    - Creates train.txt/val.txt pointing to original images\n",
    "    - Corrects class IDs in label files if needed\n",
    "    \"\"\"\n",
    "    if random_seed is not None:\n",
    "        random.seed(random_seed)\n",
    "    \n",
    "    # Define paths\n",
    "    subset_dir = os.path.join(base_dir, \"data_subset\")\n",
    "    \n",
    "    # Source paths - separate Train and val directories\n",
    "    src_train_images = os.path.join(data_root, \"Train\", \"images\")\n",
    "    src_train_labels = os.path.join(data_root, \"Train\", \"labels\")\n",
    "    src_val_images = os.path.join(data_root, \"val\", \"images\")\n",
    "    src_val_labels = os.path.join(data_root, \"val\", \"labels\")\n",
    "    \n",
    "    # Clean up existing subset directory\n",
    "    if os.path.exists(subset_dir):\n",
    "        shutil.rmtree(subset_dir)\n",
    "    os.makedirs(subset_dir, exist_ok=True)\n",
    "    \n",
    "    # Lists to store image paths\n",
    "    train_image_paths = []\n",
    "    val_image_paths = []\n",
    "    labels_corrected = 0\n",
    "    \n",
    "    total_train = 0\n",
    "    total_val = 0\n",
    "    \n",
    "    print(\"Sampling from TRAINING dataset:\")\n",
    "    for cls_name, cls_id in classes.items():\n",
    "        # --- Training data (from Train directory) ---\n",
    "        src_cls_images = os.path.join(src_train_images, cls_name)\n",
    "        src_cls_labels = os.path.join(src_train_labels, cls_name)\n",
    "        \n",
    "        if os.path.exists(src_cls_images):\n",
    "            image_files = [f for f in os.listdir(src_cls_images) if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
    "            \n",
    "            # Sample training images\n",
    "            if len(image_files) > train_samples_per_class:\n",
    "                image_files = random.sample(image_files, train_samples_per_class)\n",
    "            \n",
    "            for img_file in image_files:\n",
    "                base_name = os.path.splitext(img_file)[0]\n",
    "                original_img_path = os.path.join(src_cls_images, img_file)\n",
    "                train_image_paths.append(original_img_path)\n",
    "                \n",
    "                # Correct label file if needed\n",
    "                label_file = base_name + \".txt\"\n",
    "                label_path = os.path.join(src_cls_labels, label_file)\n",
    "                \n",
    "                if os.path.exists(label_path):\n",
    "                    with open(label_path, 'r') as f:\n",
    "                        lines = f.readlines()\n",
    "                    \n",
    "                    new_lines = []\n",
    "                    needs_correction = False\n",
    "                    for line in lines:\n",
    "                        parts = line.strip().split()\n",
    "                        if len(parts) > 1:\n",
    "                            if parts[0] != str(cls_id):\n",
    "                                needs_correction = True\n",
    "                                parts[0] = str(cls_id)\n",
    "                            new_lines.append(' '.join(parts) + '\\n')\n",
    "                    \n",
    "                    if needs_correction:\n",
    "                        with open(label_path, 'w') as f:\n",
    "                            f.writelines(new_lines)\n",
    "                        labels_corrected += 1\n",
    "            \n",
    "            total_train += len(image_files)\n",
    "            print(f\"  {cls_name}: {len(image_files)} images\")\n",
    "    \n",
    "    print(f\"\\nSampling from VALIDATION dataset:\")\n",
    "    for cls_name, cls_id in classes.items():\n",
    "        # --- Validation data (from val directory - NOT from Train) ---\n",
    "        src_cls_val_images = os.path.join(src_val_images, cls_name)\n",
    "        src_cls_val_labels = os.path.join(src_val_labels, cls_name)\n",
    "        \n",
    "        if os.path.exists(src_cls_val_images):\n",
    "            val_files = [f for f in os.listdir(src_cls_val_images) if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
    "            \n",
    "            # Sample validation images\n",
    "            if len(val_files) > val_samples_per_class:\n",
    "                val_files = random.sample(val_files, val_samples_per_class)\n",
    "            \n",
    "            for img_file in val_files:\n",
    "                base_name = os.path.splitext(img_file)[0]\n",
    "                original_img_path = os.path.join(src_cls_val_images, img_file)\n",
    "                val_image_paths.append(original_img_path)\n",
    "                \n",
    "                # Correct label file if needed\n",
    "                label_file = base_name + \".txt\"\n",
    "                label_path = os.path.join(src_cls_val_labels, label_file)\n",
    "                \n",
    "                if os.path.exists(label_path):\n",
    "                    with open(label_path, 'r') as f:\n",
    "                        lines = f.readlines()\n",
    "                    \n",
    "                    new_lines = []\n",
    "                    needs_correction = False\n",
    "                    for line in lines:\n",
    "                        parts = line.strip().split()\n",
    "                        if len(parts) > 1:\n",
    "                            if parts[0] != str(cls_id):\n",
    "                                needs_correction = True\n",
    "                                parts[0] = str(cls_id)\n",
    "                            new_lines.append(' '.join(parts) + '\\n')\n",
    "                    \n",
    "                    if needs_correction:\n",
    "                        with open(label_path, 'w') as f:\n",
    "                            f.writelines(new_lines)\n",
    "                        labels_corrected += 1\n",
    "            \n",
    "            total_val += len(val_files)\n",
    "            print(f\"  {cls_name}: {len(val_files)} images\")\n",
    "    \n",
    "    # Write train.txt\n",
    "    train_txt_path = os.path.join(subset_dir, \"train.txt\")\n",
    "    with open(train_txt_path, 'w') as f:\n",
    "        for img_path in train_image_paths:\n",
    "            f.write(img_path + '\\n')\n",
    "    \n",
    "    # Write val.txt\n",
    "    val_txt_path = os.path.join(subset_dir, \"val.txt\")\n",
    "    with open(val_txt_path, 'w') as f:\n",
    "        for img_path in val_image_paths:\n",
    "            f.write(img_path + '\\n')\n",
    "    \n",
    "    # Create data.yaml\n",
    "    data_yaml_path = os.path.join(subset_dir, \"data.yaml\")\n",
    "    data_config = {\n",
    "        'path': data_root,\n",
    "        'train': train_txt_path,\n",
    "        'val': val_txt_path,\n",
    "        'nc': len(classes),\n",
    "        'names': {v: k for k, v in classes.items()}\n",
    "    }\n",
    "    \n",
    "    with open(data_yaml_path, 'w') as f:\n",
    "        yaml.dump(data_config, f, default_flow_style=False)\n",
    "    \n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Sampled dataset created:\")\n",
    "    print(f\"  Training images: {total_train} (from Train/)\")\n",
    "    print(f\"  Validation images: {total_val} (from val/)\")\n",
    "    print(f\"  Labels corrected: {labels_corrected}\")\n",
    "    print(f\"  Config: {data_yaml_path}\")\n",
    "    print(f\"{'='*50}\")\n",
    "    \n",
    "    return data_yaml_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using FULL DATASET\n",
      "\n",
      "Training: C:\\D drive\\mydata\\MSML\\DataSets\\Raabin_datsets_withlabels\\Train\\images\n",
      "Validation: C:\\D drive\\mydata\\MSML\\DataSets\\Raabin_datsets_withlabels\\val\\images\n",
      "\n",
      "Data config: C:\\D drive\\mydata\\MSML\\GitHub\\RT-DETR-Based-Explainable-CAD-System-for-Automated-Detection-and-Classification-of-White-Blood-Cells\\output\\data_full.yaml\n"
     ]
    }
   ],
   "source": [
    "# Create data configuration\n",
    "if USE_FULL_DATASET:\n",
    "    # Use full dataset - point directly to original Train and val directories\n",
    "    print(\"Using FULL DATASET\\n\")\n",
    "    print(f\"Training: {TRAIN_IMAGES_DIR}\")\n",
    "    print(f\"Validation: {VAL_IMAGES_DIR}\")\n",
    "    \n",
    "    data_yaml_path = os.path.join(BASE_DIR, \"data_full.yaml\")\n",
    "    data_config = {\n",
    "        'path': DATA_ROOT,\n",
    "        'train': 'Train/images',\n",
    "        'val': 'val/images',\n",
    "        'nc': NUM_CLASSES,\n",
    "        'names': ID2LABEL\n",
    "    }\n",
    "    \n",
    "    with open(data_yaml_path, 'w') as f:\n",
    "        yaml.dump(data_config, f, default_flow_style=False)\n",
    "    \n",
    "    DATA_YAML = data_yaml_path\n",
    "    print(f\"\\nData config: {DATA_YAML}\")\n",
    "else:\n",
    "    # Create sampled subset from both Train and val directories\n",
    "    print(f\"Creating SAMPLED dataset...\")\n",
    "    print(f\"  Train samples: {TRAIN_SAMPLE_SIZE} per class\")\n",
    "    print(f\"  Val samples: {VAL_SAMPLE_SIZE} per class\\n\")\n",
    "    \n",
    "    DATA_YAML = create_sampled_dataset(\n",
    "        DATA_ROOT, \n",
    "        BASE_DIR, \n",
    "        CLASSES, \n",
    "        train_samples_per_class=TRAIN_SAMPLE_SIZE,\n",
    "        val_samples_per_class=VAL_SAMPLE_SIZE,\n",
    "        random_seed=42\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model_yaml_path, model_name, data_yaml, training_config, base_dir,\n",
    "                use_full_dataset=True, checkpoint_dir=None):\n",
    "    \"\"\"\n",
    "    Train the RT-DETR model and return training results.\n",
    "    \n",
    "    For full dataset training:\n",
    "      - Checks for existing checkpoint and resumes from it\n",
    "      - Tracks total epochs across multiple training sessions\n",
    "      - Saves checkpoint after training completes\n",
    "      - Uses single fixed folder per model (no timestamps)\n",
    "    \n",
    "    For sampled dataset training:\n",
    "      - Always starts fresh (no resume)\n",
    "      - Uses single fixed folder per model (no timestamps)\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Training: {model_name}\")\n",
    "    print(f\"{'='*60}\")\n",
    "\n",
    "    # Project directory\n",
    "    project_dir = os.path.join(base_dir, \"training_runs\")\n",
    "    os.makedirs(project_dir, exist_ok=True)\n",
    "\n",
    "    # Fixed run name (no timestamp) - one folder per model\n",
    "    run_name = model_name\n",
    "    run_dir = os.path.join(project_dir, run_name)\n",
    "\n",
    "    # Initialize resume tracking\n",
    "    resume_from_checkpoint = False\n",
    "    previous_epochs = 0\n",
    "    checkpoint_model_path = None\n",
    "    checkpoint_meta_path = None\n",
    "    \n",
    "    if use_full_dataset and checkpoint_dir is not None:\n",
    "        checkpoint_model_path = os.path.join(checkpoint_dir, \"last.pt\")\n",
    "        checkpoint_meta_path = os.path.join(checkpoint_dir, \"training_meta.json\")\n",
    "        \n",
    "        # Check for existing checkpoint\n",
    "        if os.path.exists(checkpoint_model_path) and os.path.exists(checkpoint_meta_path):\n",
    "            with open(checkpoint_meta_path, 'r') as f:\n",
    "                meta = json.load(f)\n",
    "            previous_epochs = meta.get('total_epochs', 0)\n",
    "            resume_from_checkpoint = True\n",
    "            print(f\"\\n*** RESUMING FROM CHECKPOINT ***\")\n",
    "            print(f\"  Previous epochs completed: {previous_epochs}\")\n",
    "            print(f\"  This session will train epochs: {previous_epochs + 1} to {previous_epochs + training_config['epochs']}\")\n",
    "            print(f\"  Loading model from: {checkpoint_model_path}\")\n",
    "        else:\n",
    "            print(f\"\\nNo checkpoint found. Starting fresh training.\")\n",
    "            # Clear existing run folder for fresh start\n",
    "            if os.path.exists(run_dir):\n",
    "                print(f\"Clearing previous run folder: {run_name}\")\n",
    "                shutil.rmtree(run_dir)\n",
    "    else:\n",
    "        # Sampled dataset - always start fresh\n",
    "        print(f\"\\nSampled dataset mode - starting fresh training.\")\n",
    "        # Clear existing run folder\n",
    "        if os.path.exists(run_dir):\n",
    "            print(f\"Clearing previous run folder: {run_name}\")\n",
    "            shutil.rmtree(run_dir)\n",
    "\n",
    "    # Load model - either from checkpoint or from YAML\n",
    "    if resume_from_checkpoint:\n",
    "        model = RTDETR(checkpoint_model_path)\n",
    "    else:\n",
    "        model = RTDETR(model_yaml_path)\n",
    "\n",
    "    # Record start time\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Train - use exist_ok=True to allow updating existing folder\n",
    "    results = model.train(\n",
    "        data=data_yaml,\n",
    "        epochs=training_config[\"epochs\"],\n",
    "        imgsz=training_config[\"imgsz\"],\n",
    "        batch=training_config[\"batch\"],\n",
    "        lr0=training_config[\"lr0\"],\n",
    "        lrf=training_config[\"lrf\"],\n",
    "        momentum=training_config[\"momentum\"],\n",
    "        weight_decay=training_config[\"weight_decay\"],\n",
    "        workers=training_config[\"workers\"],\n",
    "        patience=training_config[\"patience\"],\n",
    "        cos_lr=training_config[\"cos_lr\"],\n",
    "        warmup_epochs=training_config.get(\"warmup_epochs\", 3) if not resume_from_checkpoint else 0,\n",
    "        warmup_momentum=training_config.get(\"warmup_momentum\", 0.8),\n",
    "        warmup_bias_lr=training_config.get(\"warmup_bias_lr\", 0.1),\n",
    "        project=project_dir,\n",
    "        name=run_name,\n",
    "        exist_ok=True,\n",
    "        save=True,\n",
    "        plots=True,\n",
    "        verbose=True,\n",
    "    )\n",
    "    \n",
    "    training_time = time.time() - start_time\n",
    "    \n",
    "    # Get model paths from this run\n",
    "    best_model_path = os.path.join(run_dir, \"weights\", \"best.pt\")\n",
    "    last_model_path = os.path.join(run_dir, \"weights\", \"last.pt\")\n",
    "    \n",
    "    # Calculate total epochs\n",
    "    total_epochs = previous_epochs + training_config[\"epochs\"]\n",
    "    \n",
    "    # Save checkpoint for full dataset training\n",
    "    if use_full_dataset and checkpoint_dir is not None:\n",
    "        os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "        \n",
    "        # Copy last.pt to checkpoint directory\n",
    "        if os.path.exists(last_model_path):\n",
    "            shutil.copy2(last_model_path, checkpoint_model_path)\n",
    "            print(f\"\\nCheckpoint saved: {checkpoint_model_path}\")\n",
    "        \n",
    "        # Save/update metadata\n",
    "        meta = {\n",
    "            'total_epochs': total_epochs,\n",
    "            'last_run_epochs': training_config[\"epochs\"],\n",
    "            'last_run_dir': run_dir,\n",
    "            'last_run_time': datetime.now().isoformat(),\n",
    "            'training_sessions': 1,\n",
    "        }\n",
    "        \n",
    "        # Load existing meta to preserve history\n",
    "        if resume_from_checkpoint:\n",
    "            with open(checkpoint_meta_path, 'r') as f:\n",
    "                old_meta = json.load(f)\n",
    "            meta['training_sessions'] = old_meta.get('training_sessions', 0) + 1\n",
    "            meta['history'] = old_meta.get('history', [])\n",
    "            meta['history'].append({\n",
    "                'session': meta['training_sessions'],\n",
    "                'epochs_this_session': training_config[\"epochs\"],\n",
    "                'total_epochs_after': total_epochs,\n",
    "                'run_dir': run_dir,\n",
    "                'timestamp': datetime.now().isoformat(),\n",
    "            })\n",
    "        else:\n",
    "            meta['history'] = [{\n",
    "                'session': 1,\n",
    "                'epochs_this_session': training_config[\"epochs\"],\n",
    "                'total_epochs_after': total_epochs,\n",
    "                'run_dir': run_dir,\n",
    "                'timestamp': datetime.now().isoformat(),\n",
    "            }]\n",
    "        \n",
    "        with open(checkpoint_meta_path, 'w') as f:\n",
    "            json.dump(meta, f, indent=2)\n",
    "        print(f\"Metadata saved: {checkpoint_meta_path}\")\n",
    "        \n",
    "        print(f\"\\n*** TOTAL EPOCHS COMPLETED: {total_epochs} ***\")\n",
    "    \n",
    "    return {\n",
    "        \"model_name\": model_name,\n",
    "        \"best_model_path\": best_model_path,\n",
    "        \"training_time\": training_time,\n",
    "        \"run_dir\": run_dir,\n",
    "        \"results\": results,\n",
    "        \"total_epochs\": total_epochs,\n",
    "        \"previous_epochs\": previous_epochs,\n",
    "        \"resumed\": resume_from_checkpoint,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Training: RT-DETR-MobileNet\n",
      "============================================================\n",
      "\n",
      "No checkpoint found. Starting fresh training.\n",
      "New https://pypi.org/project/ultralytics/8.4.9 available  Update with 'pip install -U ultralytics'\n",
      "Ultralytics 8.4.8  Python-3.12.10 torch-2.6.0+cu124 CUDA:0 (NVIDIA GeForce RTX 4060 Ti, 8188MiB)\n",
      "\u001b[34m\u001b[1mengine\\trainer: \u001b[0magnostic_nms=False, amp=True, angle=1.0, augment=False, auto_augment=randaugment, batch=8, bgr=0.0, box=7.5, cache=False, cfg=None, classes=None, close_mosaic=10, cls=0.5, compile=False, conf=None, copy_paste=0.0, copy_paste_mode=flip, cos_lr=True, cutmix=0.0, data=C:\\D drive\\mydata\\MSML\\GitHub\\RT-DETR-Based-Explainable-CAD-System-for-Automated-Detection-and-Classification-of-White-Blood-Cells\\output\\data_full.yaml, degrees=0.0, deterministic=True, device=None, dfl=1.5, dnn=False, dropout=0.0, dynamic=False, embed=None, end2end=None, epochs=1, erasing=0.4, exist_ok=True, fliplr=0.5, flipud=0.0, format=torchscript, fraction=1.0, freeze=None, half=False, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, imgsz=640, int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=0.0001, lrf=0.01, mask_ratio=4, max_det=300, mixup=0.0, mode=train, model=C:\\D drive\\mydata\\MSML\\GitHub\\RT-DETR-Based-Explainable-CAD-System-for-Automated-Detection-and-Classification-of-White-Blood-Cells\\rtdetr_mobilenetv3.yaml, momentum=0.937, mosaic=1.0, multi_scale=0.0, name=RT-DETR-MobileNet, nbs=64, nms=False, opset=None, optimize=False, optimizer=auto, overlap_mask=True, patience=20, perspective=0.0, plots=True, pose=12.0, pretrained=True, profile=False, project=C:\\D drive\\mydata\\MSML\\GitHub\\RT-DETR-Based-Explainable-CAD-System-for-Automated-Detection-and-Classification-of-White-Blood-Cells\\output\\training_runs, rect=False, resume=False, retina_masks=False, rle=1.0, save=True, save_conf=False, save_crop=False, save_dir=C:\\D drive\\mydata\\MSML\\GitHub\\RT-DETR-Based-Explainable-CAD-System-for-Automated-Detection-and-Classification-of-White-Blood-Cells\\output\\training_runs\\RT-DETR-MobileNet, save_frames=False, save_json=False, save_period=-1, save_txt=False, scale=0.5, seed=0, shear=0.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=detect, time=None, tracker=botsort.yaml, translate=0.1, val=True, verbose=True, vid_stride=1, visualize=False, warmup_bias_lr=0.01, warmup_epochs=3, warmup_momentum=0.8, weight_decay=0.0005, workers=8, workspace=None\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
      "  1                  -1  1       176  ultralytics.nn.modules.conv.DWConv           [16, 16, 3, 1]                \n",
      "  2                  -1  1       288  ultralytics.nn.modules.conv.Conv             [16, 16, 1, 1]                \n",
      "  3                  -1  1      1152  ultralytics.nn.modules.conv.Conv             [16, 64, 1, 1]                \n",
      "  4                  -1  1       704  ultralytics.nn.modules.conv.DWConv           [64, 64, 3, 2]                \n",
      "  5                  -1  1      1584  ultralytics.nn.modules.conv.Conv             [64, 24, 1, 1]                \n",
      "  6                  -1  1      1872  ultralytics.nn.modules.conv.Conv             [24, 72, 1, 1]                \n",
      "  7                  -1  1       792  ultralytics.nn.modules.conv.DWConv           [72, 72, 3, 1]                \n",
      "  8                  -1  1      1776  ultralytics.nn.modules.conv.Conv             [72, 24, 1, 1]                \n",
      "  9                  -1  1      1872  ultralytics.nn.modules.conv.Conv             [24, 72, 1, 1]                \n",
      " 10                  -1  1      1944  ultralytics.nn.modules.conv.DWConv           [72, 72, 5, 2]                \n",
      " 11                  -1  1      2960  ultralytics.nn.modules.conv.Conv             [72, 40, 1, 1]                \n",
      " 12                  -1  2     19520  ultralytics.nn.modules.block.C2f             [40, 40, 2]                   \n",
      " 13                  -1  1      5040  ultralytics.nn.modules.conv.Conv             [40, 120, 1, 1]               \n",
      " 14                  -1  1      3240  ultralytics.nn.modules.conv.DWConv           [120, 120, 5, 2]              \n",
      " 15                  -1  1      9760  ultralytics.nn.modules.conv.Conv             [120, 80, 1, 1]               \n",
      " 16                  -1  3    109600  ultralytics.nn.modules.block.C2f             [80, 80, 3]                   \n",
      " 17                  -1  1     19680  ultralytics.nn.modules.conv.Conv             [80, 240, 1, 1]               \n",
      " 18                  -1  1      6480  ultralytics.nn.modules.conv.DWConv           [240, 240, 5, 2]              \n",
      " 19                  -1  1     38720  ultralytics.nn.modules.conv.Conv             [240, 160, 1, 1]              \n",
      " 20                  -1  2    308480  ultralytics.nn.modules.block.C2f             [160, 160, 2]                 \n",
      " 21                  -1  1     41472  ultralytics.nn.modules.conv.Conv             [160, 256, 1, 1]              \n",
      " 22                  -1  1    789760  ultralytics.nn.modules.transformer.AIFI      [256, 1024, 8]                \n",
      " 23                  -1  1     66048  ultralytics.nn.modules.conv.Conv             [256, 256, 1, 1]              \n",
      " 24                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 25                  16  1     20992  ultralytics.nn.modules.conv.Conv             [80, 256, 1, 1]               \n",
      " 26            [-2, -1]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 27                  -1  3   2232320  ultralytics.nn.modules.block.RepC3           [512, 256, 3]                 \n",
      " 28                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 29                  12  1     10752  ultralytics.nn.modules.conv.Conv             [40, 256, 1, 1]               \n",
      " 30            [-2, -1]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 31                  -1  3   2232320  ultralytics.nn.modules.block.RepC3           [512, 256, 3]                 \n",
      " 32                  -1  1    590336  ultralytics.nn.modules.conv.Conv             [256, 256, 3, 2]              \n",
      " 33            [-1, 27]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 34                  -1  3   2232320  ultralytics.nn.modules.block.RepC3           [512, 256, 3]                 \n",
      " 35                  -1  1    590336  ultralytics.nn.modules.conv.Conv             [256, 256, 3, 2]              \n",
      " 36            [-1, 23]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 37                  -1  3   2232320  ultralytics.nn.modules.block.RepC3           [512, 256, 3]                 \n",
      " 38        [31, 34, 37]  1   3922820  ultralytics.nn.modules.head.RTDETRDecoder    [5, [256, 256, 256], 256, 300, 4, 8, 3]\n",
      "rtdetr_mobilenetv3 summary: 279 layers, 15,497,900 parameters, 15,497,900 gradients, 57.2 GFLOPs\n",
      "\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks...\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed \n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access  (ping: 0.10.0 ms, read: 171.546.5 MB/s, size: 29.9 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mtrain: \u001b[0mScanning C:\\D drive\\mydata\\MSML\\DataSets\\Raabin_datsets_withlabels\\Train\\labels\\Basophil.cache... 10166 images, 11 backgrounds, 0 corrupt: 100% ━━━━━━━━━━━━ 10176/10176  0.0s\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access  (ping: 0.10.0 ms, read: 112.761.8 MB/s, size: 34.2 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\D drive\\mydata\\MSML\\DataSets\\Raabin_datsets_withlabels\\val\\labels\\Basophil.cache... 4261 images, 0 backgrounds, 0 corrupt: 100% ━━━━━━━━━━━━ 4261/4261  0.0s\n",
      "Plotting labels to C:\\D drive\\mydata\\MSML\\GitHub\\RT-DETR-Based-Explainable-CAD-System-for-Automated-Detection-and-Classification-of-White-Blood-Cells\\output\\training_runs\\RT-DETR-MobileNet\\labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.0001' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.001111, momentum=0.9) with parameter groups 91 weight(decay=0.0), 127 weight(decay=0.0005), 138 bias(decay=0.0)\n",
      "Image sizes 640 train, 640 val\n",
      "Using 8 dataloader workers\n",
      "Logging results to \u001b[1mC:\\D drive\\mydata\\MSML\\GitHub\\RT-DETR-Based-Explainable-CAD-System-for-Automated-Detection-and-Classification-of-White-Blood-Cells\\output\\training_runs\\RT-DETR-MobileNet\u001b[0m\n",
      "Starting training for 1 epochs...\n",
      "\n",
      "      Epoch    GPU_mem  giou_loss   cls_loss    l1_loss  Instances       Size\n",
      "\u001b[K        1/1      3.84G      1.906      17.77       1.39         29        640: 0% ──────────── 0/1272  1.4s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sriva\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\autograd\\graph.py:823: UserWarning: grid_sampler_2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\Context.cpp:96.)\n",
      "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[K        1/1      3.98G     0.9762      1.523     0.6269         18        640: 100% ━━━━━━━━━━━━ 1272/1272 3.0it/s 7:00<0.3s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 267/267 5.4it/s 49.3s<0.2s\n",
      "                   all       4261       6074       0.36      0.371      0.316      0.262\n",
      "\n",
      "1 epochs completed in 0.131 hours.\n",
      "Optimizer stripped from C:\\D drive\\mydata\\MSML\\GitHub\\RT-DETR-Based-Explainable-CAD-System-for-Automated-Detection-and-Classification-of-White-Blood-Cells\\output\\training_runs\\RT-DETR-MobileNet\\weights\\last.pt, 31.4MB\n",
      "Optimizer stripped from C:\\D drive\\mydata\\MSML\\GitHub\\RT-DETR-Based-Explainable-CAD-System-for-Automated-Detection-and-Classification-of-White-Blood-Cells\\output\\training_runs\\RT-DETR-MobileNet\\weights\\best.pt, 31.4MB\n",
      "\n",
      "Validating C:\\D drive\\mydata\\MSML\\GitHub\\RT-DETR-Based-Explainable-CAD-System-for-Automated-Detection-and-Classification-of-White-Blood-Cells\\output\\training_runs\\RT-DETR-MobileNet\\weights\\best.pt...\n",
      "Ultralytics 8.4.8  Python-3.12.10 torch-2.6.0+cu124 CUDA:0 (NVIDIA GeForce RTX 4060 Ti, 8188MiB)\n",
      "rtdetr_mobilenetv3 summary: 167 layers, 14,695,956 parameters, 0 gradients, 52.9 GFLOPs\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 267/267 6.5it/s 41.1s<0.2s\n",
      "                   all       4261       6074      0.358      0.372      0.315      0.261\n",
      "              Basophil         71        137      0.525     0.0566      0.133      0.106\n",
      "            Eosinophil        305        627     0.0446     0.0287     0.0268     0.0188\n",
      "            Lymphocyte       1017       1124      0.622      0.807      0.794      0.704\n",
      "              Monocyte        217        239      0.307      0.118     0.0941     0.0761\n",
      "            Neutrophil       2651       3947      0.292      0.851      0.525      0.399\n",
      "Speed: 0.3ms preprocess, 4.5ms inference, 0.0ms loss, 0.8ms postprocess per image\n",
      "Results saved to \u001b[1mC:\\D drive\\mydata\\MSML\\GitHub\\RT-DETR-Based-Explainable-CAD-System-for-Automated-Detection-and-Classification-of-White-Blood-Cells\\output\\training_runs\\RT-DETR-MobileNet\u001b[0m\n",
      "\n",
      "Checkpoint saved: C:\\D drive\\mydata\\MSML\\GitHub\\RT-DETR-Based-Explainable-CAD-System-for-Automated-Detection-and-Classification-of-White-Blood-Cells\\output\\checkpoints\\RT-DETR-MobileNet\\last.pt\n",
      "Metadata saved: C:\\D drive\\mydata\\MSML\\GitHub\\RT-DETR-Based-Explainable-CAD-System-for-Automated-Detection-and-Classification-of-White-Blood-Cells\\output\\checkpoints\\RT-DETR-MobileNet\\training_meta.json\n",
      "\n",
      "*** TOTAL EPOCHS COMPLETED: 1 ***\n",
      "\n",
      "Training completed in 602.8s\n",
      "Best model saved to: C:\\D drive\\mydata\\MSML\\GitHub\\RT-DETR-Based-Explainable-CAD-System-for-Automated-Detection-and-Classification-of-White-Blood-Cells\\output\\training_runs\\RT-DETR-MobileNet\\weights\\best.pt\n",
      "Total epochs trained: 1\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "training_result = train_model(\n",
    "    MODEL_YAML_PATH,\n",
    "    MODEL_NAME,\n",
    "    DATA_YAML,\n",
    "    TRAINING_CONFIG,\n",
    "    BASE_DIR,\n",
    "    use_full_dataset=USE_FULL_DATASET,\n",
    "    checkpoint_dir=CHECKPOINT_DIR if USE_FULL_DATASET else None\n",
    ")\n",
    "\n",
    "print(f\"\\nTraining completed in {training_result['training_time']:.1f}s\")\n",
    "print(f\"Best model saved to: {training_result['best_model_path']}\")\n",
    "\n",
    "if training_result['resumed']:\n",
    "    print(f\"\\nResumed from epoch {training_result['previous_epochs'] + 1}\")\n",
    "print(f\"Total epochs trained: {training_result['total_epochs']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model_path, images_dir, classes, id2label, \n",
    "                   conf_thresh=0.1, eval_per_class=100, random_seed=123):\n",
    "    \"\"\"\n",
    "    Evaluate the trained model on the dataset.\n",
    "    \"\"\"\n",
    "    model = RTDETR(model_path)\n",
    "    \n",
    "    random.seed(random_seed)\n",
    "    \n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    inference_times = []\n",
    "    \n",
    "    for gt_class, gt_id in classes.items():\n",
    "        cls_dir = os.path.join(images_dir, gt_class)\n",
    "        files = [f for f in os.listdir(cls_dir) if f.lower().endswith(\".jpg\")]\n",
    "        \n",
    "        if len(files) > eval_per_class:\n",
    "            files = random.sample(files, eval_per_class)\n",
    "        \n",
    "        for fname in tqdm(files, desc=f\"Evaluating {gt_class}\", leave=False):\n",
    "            img_path = os.path.join(cls_dir, fname)\n",
    "            \n",
    "            start = time.time()\n",
    "            results = model(img_path, conf=conf_thresh, verbose=False)[0]\n",
    "            inference_times.append(time.time() - start)\n",
    "            \n",
    "            y_true.append(gt_id)\n",
    "            \n",
    "            if len(results.boxes) == 0:\n",
    "                y_pred.append(-1)\n",
    "            else:\n",
    "                best_idx = results.boxes.conf.argmax()\n",
    "                y_pred.append(int(results.boxes.cls[best_idx].cpu().item()))\n",
    "    \n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    valid = y_pred != -1\n",
    "    valid_count = np.sum(valid)\n",
    "    \n",
    "    if valid_count > 0:\n",
    "        accuracy = accuracy_score(y_true[valid], y_pred[valid])\n",
    "        cm = confusion_matrix(y_true[valid], y_pred[valid], labels=list(range(len(classes))))\n",
    "        report = classification_report(\n",
    "            y_true[valid], y_pred[valid],\n",
    "            target_names=list(classes.keys()),\n",
    "            labels=list(range(len(classes))),\n",
    "            zero_division=0,\n",
    "            output_dict=True\n",
    "        )\n",
    "    else:\n",
    "        accuracy = 0.0\n",
    "        cm = None\n",
    "        report = None\n",
    "    \n",
    "    return {\n",
    "        \"accuracy\": accuracy,\n",
    "        \"no_prediction_count\": len(y_true) - valid_count,\n",
    "        \"total_samples\": len(y_true),\n",
    "        \"confusion_matrix\": cm.tolist() if cm is not None else None,\n",
    "        \"classification_report\": report,\n",
    "        \"avg_inference_time\": np.mean(inference_times),\n",
    "        \"y_true\": y_true.tolist(),\n",
    "        \"y_pred\": y_pred.tolist(),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating: RT-DETR-MobileNet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Results:\n",
      "  Accuracy: 0.3820\n",
      "  Avg inference time: 30.36ms\n",
      "  No predictions: 0/500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "CONF_THRESH = 0.1\n",
    "EVAL_PER_CLASS = 100\n",
    "\n",
    "print(f\"Evaluating: {MODEL_NAME}\")\n",
    "evaluation_result = evaluate_model(\n",
    "    model_path=training_result[\"best_model_path\"],\n",
    "    images_dir=IMAGES_DIR,\n",
    "    classes=CLASSES,\n",
    "    id2label=ID2LABEL,\n",
    "    conf_thresh=CONF_THRESH,\n",
    "    eval_per_class=EVAL_PER_CLASS,\n",
    ")\n",
    "\n",
    "print(f\"\\nResults:\")\n",
    "print(f\"  Accuracy: {evaluation_result['accuracy']:.4f}\")\n",
    "print(f\"  Avg inference time: {evaluation_result['avg_inference_time']*1000:.2f}ms\")\n",
    "print(f\"  No predictions: {evaluation_result['no_prediction_count']}/{evaluation_result['total_samples']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- RT-DETR-MobileNet Classification Report ---\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Basophil       1.00      0.03      0.06       100\n",
      "  Eosinophil       0.08      0.01      0.02       100\n",
      "  Lymphocyte       0.88      0.79      0.83       100\n",
      "    Monocyte       0.20      0.08      0.11       100\n",
      "  Neutrophil       0.28      1.00      0.44       100\n",
      "\n",
      "    accuracy                           0.38       500\n",
      "   macro avg       0.49      0.38      0.29       500\n",
      "weighted avg       0.49      0.38      0.29       500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print classification report\n",
    "if evaluation_result[\"classification_report\"] is not None:\n",
    "    y_true = np.array(evaluation_result[\"y_true\"])\n",
    "    y_pred = np.array(evaluation_result[\"y_pred\"])\n",
    "    valid = y_pred != -1\n",
    "    \n",
    "    print(f\"\\n--- {MODEL_NAME} Classification Report ---\")\n",
    "    print(classification_report(\n",
    "        y_true[valid],\n",
    "        y_pred[valid],\n",
    "        target_names=list(CLASSES.keys()),\n",
    "        labels=list(range(NUM_CLASSES)),\n",
    "        zero_division=0\n",
    "    ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Save Results to Disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to: C:\\D drive\\mydata\\MSML\\GitHub\\RT-DETR-Based-Explainable-CAD-System-for-Automated-Detection-and-Classification-of-White-Blood-Cells\\output\\results\\RT-DETR-MobileNet_results.json\n"
     ]
    }
   ],
   "source": [
    "# Prepare results for saving (convert numpy types to native Python)\n",
    "def convert_to_native(obj):\n",
    "    \"\"\"Convert numpy types to native Python types for JSON serialization.\"\"\"\n",
    "    if isinstance(obj, np.ndarray):\n",
    "        return obj.tolist()\n",
    "    elif isinstance(obj, (np.int64, np.int32, np.int16, np.int8)):\n",
    "        return int(obj)\n",
    "    elif isinstance(obj, (np.float64, np.float32, np.float16)):\n",
    "        return float(obj)\n",
    "    elif isinstance(obj, dict):\n",
    "        return {k: convert_to_native(v) for k, v in obj.items()}\n",
    "    elif isinstance(obj, list):\n",
    "        return [convert_to_native(i) for i in obj]\n",
    "    return obj\n",
    "\n",
    "results_to_save = {\n",
    "    \"model_name\": MODEL_NAME,\n",
    "    \"backbone\": BACKBONE,\n",
    "    \"is_pretrained\": IS_PRETRAINED,\n",
    "    \"best_model_path\": training_result[\"best_model_path\"],\n",
    "    \"run_dir\": training_result[\"run_dir\"],\n",
    "    \"training_time_s\": float(training_result[\"training_time\"]),\n",
    "    \"training_config\": TRAINING_CONFIG,\n",
    "    \"total_epochs\": training_result.get(\"total_epochs\", TRAINING_CONFIG[\"epochs\"]),\n",
    "    \"resumed_training\": training_result.get(\"resumed\", False),\n",
    "    \"accuracy\": float(evaluation_result[\"accuracy\"]),\n",
    "    \"avg_inference_time_ms\": float(evaluation_result[\"avg_inference_time\"]) * 1000,\n",
    "    \"no_prediction_count\": int(evaluation_result[\"no_prediction_count\"]),\n",
    "    \"total_samples\": int(evaluation_result[\"total_samples\"]),\n",
    "    \"confusion_matrix\": convert_to_native(evaluation_result[\"confusion_matrix\"]),\n",
    "    \"classification_report\": convert_to_native(evaluation_result[\"classification_report\"]),\n",
    "    \"y_true\": convert_to_native(evaluation_result[\"y_true\"]),\n",
    "    \"y_pred\": convert_to_native(evaluation_result[\"y_pred\"]),\n",
    "    \"classes\": CLASSES,\n",
    "    \"timestamp\": datetime.now().isoformat(),\n",
    "}\n",
    "\n",
    "# Save to JSON\n",
    "results_file = os.path.join(RESULTS_DIR, f\"{MODEL_NAME}_results.json\")\n",
    "with open(results_file, 'w') as f:\n",
    "    json.dump(results_to_save, f, indent=2)\n",
    "\n",
    "print(f\"Results saved to: {results_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "TRAINING COMPLETE\n",
      "============================================================\n",
      "Model: RT-DETR-MobileNet (MobileNetV3-Small)\n",
      "Total Epochs: 1\n",
      "Accuracy: 0.3820\n",
      "Inference Time: 30.36ms\n",
      "Training Time (this session): 602.8s\n",
      "\n",
      "Best model: C:\\D drive\\mydata\\MSML\\GitHub\\RT-DETR-Based-Explainable-CAD-System-for-Automated-Detection-and-Classification-of-White-Blood-Cells\\output\\training_runs\\RT-DETR-MobileNet\\weights\\best.pt\n",
      "Checkpoint: C:\\D drive\\mydata\\MSML\\GitHub\\RT-DETR-Based-Explainable-CAD-System-for-Automated-Detection-and-Classification-of-White-Blood-Cells\\output\\checkpoints\\RT-DETR-MobileNet\\last.pt\n",
      "Results JSON: C:\\D drive\\mydata\\MSML\\GitHub\\RT-DETR-Based-Explainable-CAD-System-for-Automated-Detection-and-Classification-of-White-Blood-Cells\\output\\results\\RT-DETR-MobileNet_results.json\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Summary\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TRAINING COMPLETE\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Model: {MODEL_NAME} ({BACKBONE})\")\n",
    "print(f\"Total Epochs: {training_result.get('total_epochs', TRAINING_CONFIG['epochs'])}\")\n",
    "if training_result.get('resumed', False):\n",
    "    print(f\"  (Resumed from epoch {training_result['previous_epochs'] + 1})\")\n",
    "print(f\"Accuracy: {evaluation_result['accuracy']:.4f}\")\n",
    "print(f\"Inference Time: {evaluation_result['avg_inference_time']*1000:.2f}ms\")\n",
    "print(f\"Training Time (this session): {training_result['training_time']:.1f}s\")\n",
    "print(f\"\\nBest model: {training_result['best_model_path']}\")\n",
    "if USE_FULL_DATASET:\n",
    "    print(f\"Checkpoint: {CHECKPOINT_MODEL_PATH}\")\n",
    "print(f\"Results JSON: {results_file}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
