{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RT-DETR MobileNet Training\n",
    "\n",
    "Training RT-DETR with MobileNetV3-Small backbone for WBC Classification on Raabin-WBC dataset.\n",
    "\n",
    "## Model Details\n",
    "- **Backbone**: MobileNetV3-Small\n",
    "- **Training**: From scratch (no pretrained weights)\n",
    "- **Dataset**: Raabin-WBC with 5 cell types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install -U ultralytics torch torchvision pillow tqdm scikit-learn seaborn timm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "%matplotlib inline\n\nimport os\nimport json\nimport yaml\nfrom datetime import datetime\n\nimport numpy as np\nimport torch\n\nfrom sklearn.metrics import classification_report\n\n# Import common training utilities\nfrom training_utils import (\n    create_sampled_dataset,\n    create_full_dataset_config,\n    train_model,\n    evaluate_model,\n    save_results,\n    print_training_summary,\n)\n\nprint(f\"PyTorch version: {torch.__version__}\")\nprint(f\"CUDA available: {torch.cuda.is_available()}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Notebook directory: C:\\D drive\\mydata\\MSML\\GitHub\\RT-DETR-Based-Explainable-CAD-System-for-Automated-Detection-and-Classification-of-White-Blood-Cells\n",
      "Base directory: C:\\D drive\\mydata\\MSML\\GitHub\\RT-DETR-Based-Explainable-CAD-System-for-Automated-Detection-and-Classification-of-White-Blood-Cells\\output\n",
      "Data root: C:\\D drive\\mydata\\MSML\\DataSets\\Raabin_datsets_withlabels\n",
      "Found model YAML: C:\\D drive\\mydata\\MSML\\GitHub\\RT-DETR-Based-Explainable-CAD-System-for-Automated-Detection-and-Classification-of-White-Blood-Cells\\rtdetr_mobilenetv3.yaml\n",
      "\n",
      "Using device: cuda\n",
      "Dataset mode: FULL DATASET\n",
      "Checkpoint directory: C:\\D drive\\mydata\\MSML\\GitHub\\RT-DETR-Based-Explainable-CAD-System-for-Automated-Detection-and-Classification-of-White-Blood-Cells\\output\\checkpoints\\RT-DETR-MobileNet\n",
      "  -> No checkpoint found. Training will start from scratch.\n",
      "\n",
      "Training data: C:\\D drive\\mydata\\MSML\\DataSets\\Raabin_datsets_withlabels\\Train\\images\n",
      "Validation data: C:\\D drive\\mydata\\MSML\\DataSets\\Raabin_datsets_withlabels\\val\\images\n",
      "\n",
      "Model: RT-DETR-MobileNet (MobileNetV3-Small)\n",
      "Training mode: From scratch\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# MODEL CONFIGURATION\n",
    "# =============================================================================\n",
    "MODEL_NAME = \"RT-DETR-MobileNet\"\n",
    "BACKBONE = \"MobileNetV3-Small\"\n",
    "IS_PRETRAINED = False  # Training from scratch\n",
    "\n",
    "# =============================================================================\n",
    "# BASE DIRECTORY\n",
    "# =============================================================================\n",
    "NOTEBOOK_DIR = os.getcwd()\n",
    "BASE_DIR = os.path.join(NOTEBOOK_DIR, \"output\")\n",
    "\n",
    "# Dataset path (contains separate Train and val folders)\n",
    "DATA_ROOT = r\"C:\\D drive\\mydata\\MSML\\DataSets\\Raabin_datsets_withlabels\"\n",
    "\n",
    "# Custom model YAML path\n",
    "MODEL_YAML_PATH = os.path.join(NOTEBOOK_DIR, \"rtdetr_mobilenetv3.yaml\")\n",
    "\n",
    "print(f\"Notebook directory: {NOTEBOOK_DIR}\")\n",
    "print(f\"Base directory: {BASE_DIR}\")\n",
    "print(f\"Data root: {DATA_ROOT}\")\n",
    "\n",
    "# Verify YAML file exists\n",
    "if os.path.exists(MODEL_YAML_PATH):\n",
    "    print(f\"Found model YAML: {MODEL_YAML_PATH}\")\n",
    "else:\n",
    "    print(f\"WARNING: Model YAML not found at: {MODEL_YAML_PATH}\")\n",
    "\n",
    "# =============================================================================\n",
    "# SAMPLING CONFIGURATION\n",
    "# =============================================================================\n",
    "USE_FULL_DATASET = True  # Set to True to use ALL images, False for sampling\n",
    "\n",
    "# Sample sizes per class (only used when USE_FULL_DATASET=False)\n",
    "TRAIN_SAMPLE_SIZE = 100   # Number of training samples per class\n",
    "VAL_SAMPLE_SIZE = 20      # Number of validation samples per class\n",
    "\n",
    "# =============================================================================\n",
    "# CHECKPOINT CONFIGURATION (for resume training on full dataset)\n",
    "# =============================================================================\n",
    "CHECKPOINT_DIR = os.path.join(BASE_DIR, \"checkpoints\", MODEL_NAME)\n",
    "CHECKPOINT_MODEL_PATH = os.path.join(CHECKPOINT_DIR, \"last.pt\")\n",
    "CHECKPOINT_META_PATH = os.path.join(CHECKPOINT_DIR, \"training_meta.json\")\n",
    "\n",
    "# Create checkpoint directory\n",
    "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
    "\n",
    "# Data paths (separate train and validation directories)\n",
    "TRAIN_IMAGES_DIR = os.path.join(DATA_ROOT, \"Train\", \"images\")\n",
    "TRAIN_LABELS_DIR = os.path.join(DATA_ROOT, \"Train\", \"labels\")\n",
    "VAL_IMAGES_DIR = os.path.join(DATA_ROOT, \"val\", \"images\")\n",
    "VAL_LABELS_DIR = os.path.join(DATA_ROOT, \"val\", \"labels\")\n",
    "\n",
    "# For evaluation (uses training images by default)\n",
    "IMAGES_DIR = TRAIN_IMAGES_DIR\n",
    "\n",
    "# Output directories\n",
    "os.makedirs(BASE_DIR, exist_ok=True)\n",
    "MODEL_DIR = os.path.join(BASE_DIR, \"models\")\n",
    "RESULTS_DIR = os.path.join(BASE_DIR, \"results\")\n",
    "os.makedirs(MODEL_DIR, exist_ok=True)\n",
    "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
    "\n",
    "# Device configuration\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Class definitions\n",
    "CLASSES = {\n",
    "    \"Basophil\": 0,\n",
    "    \"Eosinophil\": 1,\n",
    "    \"Lymphocyte\": 2,\n",
    "    \"Monocyte\": 3,\n",
    "    \"Neutrophil\": 4\n",
    "}\n",
    "ID2LABEL = {v: k for k, v in CLASSES.items()}\n",
    "NUM_CLASSES = len(CLASSES)\n",
    "\n",
    "print(f\"\\nUsing device: {DEVICE}\")\n",
    "if USE_FULL_DATASET:\n",
    "    print(f\"Dataset mode: FULL DATASET\")\n",
    "    print(f\"Checkpoint directory: {CHECKPOINT_DIR}\")\n",
    "    # Check for existing checkpoint\n",
    "    if os.path.exists(CHECKPOINT_MODEL_PATH) and os.path.exists(CHECKPOINT_META_PATH):\n",
    "        with open(CHECKPOINT_META_PATH, 'r') as f:\n",
    "            meta = json.load(f)\n",
    "        print(f\"  -> Found existing checkpoint: {meta['total_epochs']} epochs completed\")\n",
    "        print(f\"  -> Training will RESUME from epoch {meta['total_epochs'] + 1}\")\n",
    "    else:\n",
    "        print(f\"  -> No checkpoint found. Training will start from scratch.\")\n",
    "else:\n",
    "    print(f\"Dataset mode: SAMPLED (Train: {TRAIN_SAMPLE_SIZE}/class, Val: {VAL_SAMPLE_SIZE}/class)\")\n",
    "    print(f\"  -> Sampled mode: Always starts fresh (no resume)\")\n",
    "print(f\"\\nTraining data: {TRAIN_IMAGES_DIR}\")\n",
    "print(f\"Validation data: {VAL_IMAGES_DIR}\")\n",
    "print(f\"\\nModel: {MODEL_NAME} ({BACKBONE})\")\n",
    "print(f\"Training mode: {'Pretrained' if IS_PRETRAINED else 'From scratch'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Training Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Configuration:\n",
      "============================================================\n",
      "  epochs: 1\n",
      "  imgsz: 640\n",
      "  batch: 8\n",
      "  lr0: 0.0001\n",
      "  lrf: 0.01\n",
      "  momentum: 0.937\n",
      "  weight_decay: 0.0005\n",
      "  workers: 8\n",
      "  patience: 20\n",
      "  cos_lr: True\n",
      "  warmup_epochs: 3\n",
      "  warmup_momentum: 0.8\n",
      "  warmup_bias_lr: 0.01\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# TRAINING HYPERPARAMETERS (FROM SCRATCH CONFIG)\n",
    "# =============================================================================\n",
    "# Training from scratch needs more epochs and lower learning rate\n",
    "\n",
    "TRAINING_CONFIG = {\n",
    "    \"epochs\": 1,           \n",
    "    \"imgsz\": 640,\n",
    "    \"batch\": 8,\n",
    "    \"lr0\": 0.0001,          # Low LR for stability\n",
    "    \"lrf\": 0.01,\n",
    "    \"momentum\": 0.937,\n",
    "    \"weight_decay\": 0.0005,\n",
    "    \"workers\": 8,\n",
    "    \"patience\": 20,\n",
    "    \"cos_lr\": True,\n",
    "    \"warmup_epochs\": 3,     # Gradual LR ramp-up for stability\n",
    "    \"warmup_momentum\": 0.8,\n",
    "    \"warmup_bias_lr\": 0.01,\n",
    "}\n",
    "\n",
    "print(\"Training Configuration:\")\n",
    "print(\"=\"*60)\n",
    "for k, v in TRAINING_CONFIG.items():\n",
    "    print(f\"  {k}: {v}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# create_sampled_dataset is imported from training_utils.py\n# See training_utils.py for the implementation"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Create data configuration\nif USE_FULL_DATASET:\n    print(\"Using FULL DATASET\\n\")\n    print(f\"Training: {TRAIN_IMAGES_DIR}\")\n    print(f\"Validation: {VAL_IMAGES_DIR}\")\n    \n    DATA_YAML = create_full_dataset_config(DATA_ROOT, BASE_DIR, NUM_CLASSES, ID2LABEL)\n    print(f\"\\nData config: {DATA_YAML}\")\nelse:\n    print(f\"Creating SAMPLED dataset...\")\n    print(f\"  Train samples: {TRAIN_SAMPLE_SIZE} per class\")\n    print(f\"  Val samples: {VAL_SAMPLE_SIZE} per class\\n\")\n    \n    DATA_YAML = create_sampled_dataset(\n        DATA_ROOT, \n        BASE_DIR, \n        CLASSES, \n        train_samples_per_class=TRAIN_SAMPLE_SIZE,\n        val_samples_per_class=VAL_SAMPLE_SIZE,\n        random_seed=42\n    )"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# train_model is imported from training_utils.py\n# See training_utils.py for the implementation"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Train the model\ntraining_result = train_model(\n    model_source=MODEL_YAML_PATH,\n    model_name=MODEL_NAME,\n    data_yaml=DATA_YAML,\n    training_config=TRAINING_CONFIG,\n    base_dir=BASE_DIR,\n    use_full_dataset=USE_FULL_DATASET,\n    checkpoint_dir=CHECKPOINT_DIR if USE_FULL_DATASET else None,\n    default_warmup_epochs=3  # From scratch training needs more warmup\n)\n\nprint(f\"\\nTraining completed in {training_result['training_time']:.1f}s\")\nprint(f\"Best model saved to: {training_result['best_model_path']}\")\n\nif training_result['resumed']:\n    print(f\"\\nResumed from epoch {training_result['previous_epochs'] + 1}\")\nprint(f\"Total epochs trained: {training_result['total_epochs']}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# evaluate_model is imported from training_utils.py\n# See training_utils.py for the implementation"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating: RT-DETR-MobileNet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Results:\n",
      "  Accuracy: 0.3820\n",
      "  Avg inference time: 30.36ms\n",
      "  No predictions: 0/500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "CONF_THRESH = 0.1\n",
    "EVAL_PER_CLASS = 100\n",
    "\n",
    "print(f\"Evaluating: {MODEL_NAME}\")\n",
    "evaluation_result = evaluate_model(\n",
    "    model_path=training_result[\"best_model_path\"],\n",
    "    images_dir=IMAGES_DIR,\n",
    "    classes=CLASSES,\n",
    "    id2label=ID2LABEL,\n",
    "    conf_thresh=CONF_THRESH,\n",
    "    eval_per_class=EVAL_PER_CLASS,\n",
    ")\n",
    "\n",
    "print(f\"\\nResults:\")\n",
    "print(f\"  Accuracy: {evaluation_result['accuracy']:.4f}\")\n",
    "print(f\"  Avg inference time: {evaluation_result['avg_inference_time']*1000:.2f}ms\")\n",
    "print(f\"  No predictions: {evaluation_result['no_prediction_count']}/{evaluation_result['total_samples']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- RT-DETR-MobileNet Classification Report ---\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Basophil       1.00      0.03      0.06       100\n",
      "  Eosinophil       0.08      0.01      0.02       100\n",
      "  Lymphocyte       0.88      0.79      0.83       100\n",
      "    Monocyte       0.20      0.08      0.11       100\n",
      "  Neutrophil       0.28      1.00      0.44       100\n",
      "\n",
      "    accuracy                           0.38       500\n",
      "   macro avg       0.49      0.38      0.29       500\n",
      "weighted avg       0.49      0.38      0.29       500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print classification report\n",
    "if evaluation_result[\"classification_report\"] is not None:\n",
    "    y_true = np.array(evaluation_result[\"y_true\"])\n",
    "    y_pred = np.array(evaluation_result[\"y_pred\"])\n",
    "    valid = y_pred != -1\n",
    "    \n",
    "    print(f\"\\n--- {MODEL_NAME} Classification Report ---\")\n",
    "    print(classification_report(\n",
    "        y_true[valid],\n",
    "        y_pred[valid],\n",
    "        target_names=list(CLASSES.keys()),\n",
    "        labels=list(range(NUM_CLASSES)),\n",
    "        zero_division=0\n",
    "    ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Save Results to Disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Save results to JSON\nresults_file = save_results(\n    results_dir=RESULTS_DIR,\n    model_name=MODEL_NAME,\n    backbone=BACKBONE,\n    is_pretrained=IS_PRETRAINED,\n    training_result=training_result,\n    evaluation_result=evaluation_result,\n    training_config=TRAINING_CONFIG,\n    classes=CLASSES\n)\n\nprint(f\"Results saved to: {results_file}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Print training summary\nprint_training_summary(\n    model_name=MODEL_NAME,\n    backbone=BACKBONE,\n    training_result=training_result,\n    evaluation_result=evaluation_result,\n    training_config=TRAINING_CONFIG,\n    checkpoint_model_path=CHECKPOINT_MODEL_PATH if USE_FULL_DATASET else None,\n    results_file=results_file\n)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}